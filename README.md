202045801 신호및시스템 기말과제

1. 배경
1.1 개발의 필요성
현대 사회는 음성 인식 서비스의 활용도와 보급이 많이 증가하고 있습니다.
1) 스마트폰 및 스마트 스피커: 음성 명령을 통한 핸즈프리(Handsfree) 기능을 제공
2) 스마트 홈: 조명 제어, 온도 조절, 가전제품 제어 등
3) 보안 시스템: 음성 인식 기반 개인 인증 등
4 )음악 산업: 해당 곡의 아티스트 찾기 등
하지만 현재 음성 인식 서비스는 안정적인 성능을 항상 제공할 수 없습니다.
다양한 환경에서 음성의 인식은 아직 여러 가지 변수가 많아 정확한 결과를 내기 어렵습니다.
그러한 예로는 다중 발화자, 개인별 음성의 특징 차, 음성 품질 저하, 배경소음 등이 있습니다. 
다중 발화자의 경우에는 멀티채널 환경, 예를 들어 온라인 가상 회의 등의 경우에 여러 사람이 동시에 말하는 경우 음성 인식 서비스가 원하는 음성을 구분하고 분리하는 것이 어려워집니다. 
개인별 음성의 특징 차로 인해 STT(Speech To Text) 서비스를 시도하면 발화자와 관계없이 변환하여 누가 말했는지 인식하기 어렵습니다. 한 인터넷 방송인의 경우, 애플의 스마트폰 인공지능인 ‘시리’를 자신의 목소리로 등록해두었으나 TTS(Text to Speech)로 구현된 후원 메시지로 개인정보를 생방송 중에 송출하는 사고가 발생했습니다.
음성 품질 저하의 경우에는 네이버 사의 클로바 노트 앱을 통해 강의를 녹음하여 변환하려고 시도했으나, 교수님과의 거리와 디바이스의 마이크 성능, 학교 와이파이의 느린 네트워크 속도 등으로 사용할 수 없는 결과물이 나왔던 경험이 있습니다.
또한, 당시 에어컨 공사로 인해 공사 소음이 크게 발생했었기에 음성 인식에도 큰 영향을 주었다고 생각합니다.

이러한 사례와 경험으로 음성 인식 서비스의 Input 음성 데이터를 전처리하는 기술이 발전해야 AI 모델의 학습과 Output 산출에 동력을 불어넣을 것으로 생각합니다.
1.2 국내외 현황
1.2.1 국내 현황
한국전자통신연구원에 의하면, 2023년 기준 국내 음성 인식 서비스 시장 규모는 약 1,500억 원으로 예상되며, 2028년에는 4,000억 원 이상으로 성장할 전망입니다. 주요 사업자로는 네이버, 카카오, SKT, KT, LG 등이 있습니다. 음성 인식 엔진의 경우, 네이버의 'CLOVA Speech', 카카오의 'Kakao i', LG전자의 'Deep Voice', SKT의 'T Voice' 등이 있습니다.
1.2.2 국외 현황
글로벌리서치&데이터 ‘MarketsandMarkets‘에 의하면, 2023년 글로벌 음성 인식 서비스 시장 규모는 약 200억 달러로 예상되며, 2028년에는 600억 달러 이상으로 성장할 전망입니다. 특히 미국이 선진국 시장을 주도하고 있으며, 개발 도상국 또한 빠르게 성장하고 있습니다. 주요 사업자로는 구글, 아마존, 마이크로소프트, 페이스북 등이 있으며 이들은 클라우드 기반 음성 인식 서비스, 음성 인식 솔루션 등을 제공하고 있습니다.
2. 목표
	본 연구는 음성 인식 서비스를 위해 필요한 Input 데이터의 전처리를 위해 요구되는 결과와 AI 기반의 상용 음성 분리 서비스로 전처리한 데이터를 비교 분석하며 현 시점의 음성 데이터 전처리 기술 혹은 음성 필터링의 성능과 한계, 개선점을 찾고자 합니다. 이를 위해 앞에서 언급한 음성 인식에 영향을 주는 문제 상황 중 하나를 선택하여 해당 환경에서 현재 상용 서비스들의 성능을 분석하고자 합니다.
  
3. 내용 및 방법
  3.1 문제 상황 선택
	다중 발화자와 개인별 음성의 특징 차의 경우에는 표본의 선정과 데이터 가공에 있어 현재 장비로는 불가능하다 판단이 들었습니다. 음성 품질 저하의 경우에도 마찬가지로 장비의 한계와 이를 테스트하기 위한 소프트웨어의 보유 여부 등으로 인해 분석에 어려움이 있어 제외하였습니다. 배경소음의 경우에는 가지고 있던 장비와 구매한 음원 관련 소프트웨어로 구현과 데이터 가공 및 처리가 가능하다는 판단이 들어 진행했습니다.
데이터 제작 및 가공에 사용한 장비 및 소프트웨어의 목록

마이크
fifine사의 AM8 USB/XLR 다이나믹 마이크
DAW(Digital Audio Workstation)Image Line Software 사의 FL STUDIO
VST(Virtual Studio Technology)
Supertone사의 Voice Separator, Clear

 
3.2 데이터 생성 및 가공
	음원 분리 서비스의 특징을 고려하여 다음과 같은 상업, 비상업적으로 이용가능한 화자가 노래하는 음성 샘플을 사용함(사진 속 환경은 FL studio)
 https://www.looperman.com/acapellas/detail/22440/diamonds-main-vocal-74bpm-chill-out-acapella
해당 음성 데이터를 약 34초의 구간으로 자르고 소리가 없는 부분을 보정했다. 음성의 원본의 경우 MP3 형식으로 wav에 비해 압축률이 높고 음원 손실율이 높아 자연스러운 환경에서 녹음한 것과 비슷하게 Gain 출력과 Equalizer를 통해 가청 주파수 영역만 통과할 수 있게 Bandpass filter로 가공하였다. 이를 Original Vocal Sample이라 명명한다.

가공된 음성데이터를 테스트하기 위해 가상악기를 통해 음원으로 2차 가공하였다. 여러 상황을 고려하기 위하여 저 음역대와 중 음역대, 고 음역대 별로 가상 악기 소리와 영화와 같은 영상 분야에서 사용되는 가상 BGS(Back Grounds Sounds), SE(Sound Effect)를 삽입하고, 거리에 따른 자연스러움과 공간감을 위해 리버브(Reverbe)를 추가하고 Panning을 조정하였다. 이를 Vocal with instruments라 명명한다. 이후 Vocal with instruments를 Input 데이터로 사용하여 
1) Supertone 사의 Voice Seperator, Clear
2) Gaudiolab 사의 AI 음원 분리 서비스
3) FL Studio 자체 내장 음원 분리 기능
세 가지 방법을 통해 전처리된 데이터를 각각 Vocal filter with Supertone, Gaudiolab, FL이라고 명명한다.

 3.3 코드 작성
음향 데이터를 분석하기 위한 환경으로 Jupyter NoteBook을 사용하였다. 이를 위한 IDE로 Visual Studio Code를 사용했다. 음향 데이터의 분석을 위해 Librosa 라이브러리를 사용하였다. 
작성한 코드와 데이터: https://github.com/SoboRocream/signal-and-system-HW
분석과 그래프 출력을 위해 참고한 자료: https://librosa.org/doc/latest/index.html

 3.4 데이터 비교
1) Original Vocal Sample과 Vocal with instruments 비교
우선 Time domain에서 Waveform 간의 차이를 확인하면 다음과 같이 세번째 그래프의 노란색 영역만큼 차이가 발생했다. 이로서 확실하게 음성이 아닌 노이즈가 삽입되었음을 확인할 수 있다. 

2)Vocal filter with Supertone
Superton사의 VST 플러그인을 통해 전처리된 데이터의 경우, 육안으로 Waveform을 보았을 때는 Amplitude가 기존 Original Vocal Sample 보다 높게 Gain이 처리된 것을 확인할 수 있었다. 직접 들었을 때는 노이즈가 명확하게 제거되지 않은 듯 드문드문 노이즈를 귀로 들을 수 있었다.


FFT(Fast Fourier Transform) 기법으로 frequency domain으로 비교했을 때에는 큰 차이가 없다. 이는, 해당 VST 플러그인의 특성 상 편곡 작업 시 다른 악기 소리에 묻히는 일이 없도록 Gain 증폭을 통해 보정하는 과정이 있기에 줄어든 노이즈 또한 같이 증폭되었고, 그로 인해 제대로 제거되지 못한 것이라 추측한다. 

FFT는 시간 정보가 유실되기에 이를 보안하고자 STFT(Short-Time Fourier Transform) 기법으로 다시 분석해 봤다. 다음은 STFT의 결과를 Log Scale로 변환한 그래프이다.

Log Scale로 확인했을 때, 귀로 확인할 수 있었던 제거되지 못한 노이즈를 시각적으로 확인할 수 있었다. Original Vocal Sample과 비교했을 때, 대부분의 영역이 제거되지 못한 채 뿌옇게 남아있는 것을 확인할 수 있었다. 
MFCC(Mel Frequency Cepstral Coefficient)로 처리한 데이터의 특징 값을 살펴봤을 때는 필터링 성능이 좋을수록 차이 그래프에서 더 적은 특징 값이 남아야 한다. 하지만 살짝 옅어진 것을 제외하고 큰 차이를 확인할 수 없다.

3) Vocal filter with Gaudiolab
Gaudiolab 전처리 결과물의 경우에는 MP3 타입으로 산출되어 원본인 WAV 형식과 압축률과 손실율이 차이가 있어 신뢰성 있는 결과 분석을 기대하기 어려울 것으로 예상한다.

Time domain에서 Waveform의 경우 확실히 그 차이가 줄어든 모습을 볼 수 있다.
FFT 스펙트럼의 경우에도 마찬가지로 큰 차이를 확인할 수 없었다. 이로 인해 음향 데이터의 경우 시간 정보 유실에 따른 데이터 분석이 크게 좌우된다는 것을 알 수 있었다. 


마찬가지로 STFT 기법을 통해 시간 정보 유실을 최소화하여 분석한 결과는 다음과 같다.


확연히 차이가 줄어든 것을 시각적으로도 확인할 수 있었다.
다만, 전처리 과정에서 적절한 Bandpass 필터링이 적용되지 않는 듯 여전히 y값이 64이하로는 노이즈가 끼여 있는 것을 알 수 있다. MFCC로 처리한 데이터의 특징 값을 살펴봤을 때 또한, 특정 주파수 위로는 대부분의 노이즈가 제거된 것을 확인할 수 있다.
4) Vocal filter with FL STUDIO
사용하고 있는 DAW의 내장 기능으로 해당 소프트웨어의 홈페이지에서는 다음과 같이 설명하고 있다.  Extract bass, synth, drums, and vocal layers from any audio file, directly within FL Studio, using AI-powered Stem Separation.
AI 기반 분리를 사용하여 DAW 내에서 직접 모든 오디오 파일의 악기 및 보컬 레이어를 추출한다. Waveform 비교상으로는 이전의 Gaudiolab과 비슷한 결과를 보여주고 있다.


STFT 로그 스펙트럼 상에서 제거되지 못한 미세 노이즈 부분을 확인할 수 있었다.
두번째와 세번째 그래프 상에 표시된 영역은 삽입된 노이즈 중 저 음역대에 들어간 가상악기의 MIDI형식과 유사한 형태를 보이고 있다.


하지만, MFCC로 비교했을 때는 Gaudiolab의 경우와 비슷하게 특징 값이 매우 옅게 나타남을 확인할 수 있었다. 


4. 결론
여러 상용 AI 기술들의 결과물을 분석한 결과, 인간의 귀로는 크게 의식되지 않을 정도로 분리가 가능하나, Bandpass 필터 처리로 불필요한 주파수 범위의 신호를 제거하는 것과 데이터 수치 상으로 확인할 수 있는 미세한 노이즈는 잡히지 않는 모습을 보여주고 있다. 음악 산업 같은 경우에는 자연스러움과 공간감을 위해 일부러 노이즈를 넣고 여러가지 효과를 넣기 때문에 큰 문제가 없으리라 생각되지만, 음성 인식의 경우에는 저러한 수치들로 인해 AI가 더디거나 잘못된 학습이 될 가능성이 있고, 그에 따른 튜닝이 요구되기 때문에 이러한 점을 집중적으로 해결하기 위해 노력해야 할 점을 시사할 수 있었다.
5. 활용방안
  - 다중 Bandpass 필터를 통한 음성 데이터 전처리 성능 향상에 방향성 제시
- 온-디바이스 AI가 탑재된 기기에서 아날로그 필터의 추가로 Input 데이터 및 전처리 된 데이터의 효율 개선을 위한 연구
- AI를 통한 Sampling Rate 값 튜닝 및 가변 Sampling Rate

6. 기대효과
  - 음성 인식 서비스의 상황에 따른 대응력과 정확성, 인식률 향상
- 음성 인식 AI 모델의 학습 용이성
